---
title: "Hand Coded"
description: "Complex model from scratch"
dateString: ongoing
draft: false
tags: ["Pytorch","Transformer","Math"]
showToc: False
weight: 205
cover:
    image: "projects/hand_coded/coding.jpg"
--- 
### Github link
ðŸ’¾[Backend](https://github.com/HanL1223/from_scratch)
### Credit
Transformer model is introduced in [this paper](https://arxiv.org/abs/1706.03762)

### Skill invlove
**Python**  **Pytorch**  **Deep Learning**

Model 1:
## Hand Coded Transformer with Multihead Self-Attention

This is a hand-coded implementation of Transformer with Multihead Self-Attention.

The main goal is to help me better understand the underline logic of transformer model

## Overview

The Transformer model revolutionized sequence modeling by employing the self-attention mechanism. This mechanism enables the model to capture long-range dependencies efficiently, making it highly effective for processing sequential data. The core idea of the Transformer lies in its ability to process input sequences in parallel, resulting in faster computation and better capture of global information.

The key component of the Transformer model is the multihead self-attention mechanism. This mechanism allows the model to attend to different parts of the input sequence simultaneously, utilizing multiple attention heads. By leveraging multiple attention heads, the model can learn diverse representations and capture various dependencies within the sequence.

## Features

- Hand-coded implementation of the Transformer model with multihead self-attention
- Encoder-decoder architecture for sequence-to-sequence tasks
- Self-attention mechanism for capturing global dependencies
- Multihead attention for learning diverse representations
- Positional encoding to incorporate word order information
- Feed-forward networks for non-linear transformations
- Customizable hyperparameters for fine-tuning the model
- Training procedures for optimizing the model's performance

