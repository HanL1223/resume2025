
[{"content":" Github link # ðŸ’¾ Project Github\nCredit # Members of team TP8 for subject FIT5120/22\nSkill invlove # Python Pytorch Deep Learning Docker AWS Azure\nDescription # This is a automated task of determining whether a piece of text is a spam or not. In this project, I built a classifier using PyTorch to fine-tune a BERT model.\nKey Takeaways # Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Collaborative Development. Experience a full-stack project development with a team,and using Github for version control in a team setting Agile Management. Here I also exposed to the full agile software development cycle of a product Cloud Computing. In-depth knowledge and hands on experience on both AWS(EC2,Route52,S3) and Azure (Machine Learning Studio,Virtual machines) New Domain knowledge. First time expose to developing a product in a more public domain setting ","externalUrl":null,"permalink":"/project/seniordigi/","section":"Projects","summary":"","title":"Bridging the Digital Divide","type":"project"},{"content":" Github link # ðŸ’¾ Project Github\nSkill invlove # Python Machine Learning Data Visulization\nDescription # The increasing number of VISA applicants every year in US calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. The objective is to analyze the data provided and, with the help of a classification model:\nFacilitate the process of visa approvals. Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status. Figure 1 # Important feature that affect the application success rate - either positive or negative Figure 2 # Important feature that affect the application success rate - either positive or negative Key Takeaways # End to End data project involve data exploration, data manipulation and preprocessing, model selection, evaluation and fine-tune. ","externalUrl":null,"permalink":"/project/visa-granted-rate/","section":"Projects","summary":"","title":"Visa Approveal rate predictionm","type":"project"},{"content":" Github link # ðŸ”— Code\nCredit # This notebook is inspire by Paper Skill invlove # Python Collaborative Filtering Data preprocessing ScikitLearn\nProject Description # This project aims to build a recommender system for book ratings. The system utilizes collaboration filtering methods and ensemble techniques to provide accurate recommendations to users based on their preferences. The project includes data exploration, preprocessing, model selection, model construction, and evaluation phases.\nThe recommender system employs three standalone models, implementing collaboration filtering methods. The models are fine-tuned and compared using cross-validation to select the best-performing model. The chosen model is then further optimized using random search cross-validation. The top-performing models, including SVD, SVDpp, and KNNwithZscore, are used to construct an ensemble model for testing.\nProject Takeaways # Throughout the development of the Recommender System for Book Ratings, I gained several key takeaways:\nEnhanced Python skills, especially in data manipulation, model training, and evaluation. Deepened understanding of collaborative filtering techniques and their application in recommender systems. Explored data exploration techniques to identify trends and patterns in book ratings. Learned data preprocessing methods to improve data quality and reduce training time. Experienced model selection processes using cross-validation to identify the best-performing model. Practiced fine-tuning models with random search cross-validation for improved accuracy and robustness. Constructed an ensemble model by combining the top-performing models, showcasing the benefits of leveraging multiple models. Utilized evaluation metrics (MAE and RMSE) to quantitatively measure the performance of the recommender system. Identified limitations of collaborative filtering, such as cold-start issues and the presence of unseen books. Recognized the potential for future enhancements, including incorporating content-based methods and hybrid approaches. Considered scalability using frameworks like Spark to handle larger datasets and improve system performance.(Future improvement) Usage # The recommender system can be used to provide book recommendations based on user preferences. Users can input their user ID or book name to receive personalized recommendations. The system utilizes collaborative filtering techniques and an ensemble model to generate accurate recommendations.\nTo obtain recommendations for a specific user:\nimport recommender_system book_name = \u0026#34;Angels \u0026amp; Demons (Robert Langdon, #1)\u0026#34; # Name of the book for which recommendations are required recommendations(df = test_data,book_name = book_name) id user_id\trating 33038\t1783\t4 26076\t1483\t4 193\t6\t3 26074\t1541\t3 930\t65\t3 For sample output ,we look at the reading history of top user 1783\ntrain[train[\u0026#39;user_id\u0026#39;] == 1483] user_id item_id rating book_name 135608 1483 399 4 The Da Vinci Code (Robert Langdon, #2) 135673 1483 456 4 Memoirs of a Geisha 135757 1483 1074 5 1984 135885 1483 1113 4 Harry Potter and the Order of the Phoenix (Har\u0026hellip; 136250 1483 1302 5 The Devil in the White City For the 1st place recommendation, since this user readed the Da Vinci Code, they might also interesting in its prequel. So the recommendation logically make sence\n","externalUrl":null,"permalink":"/posts/recommender/","section":"Posts","summary":"","title":"Novel Recommender","type":"posts"},{"content":" Github link # ðŸ”— Code\nCredit # This notebook is inspire by Paper Skill invlove # Python Collaborative Filtering Data preprocessing ScikitLearn\nProject Description # This project aims to build a recommender system for book ratings. The system utilizes collaboration filtering methods and ensemble techniques to provide accurate recommendations to users based on their preferences. The project includes data exploration, preprocessing, model selection, model construction, and evaluation phases.\nThe recommender system employs three standalone models, implementing collaboration filtering methods. The models are fine-tuned and compared using cross-validation to select the best-performing model. The chosen model is then further optimized using random search cross-validation. The top-performing models, including SVD, SVDpp, and KNNwithZscore, are used to construct an ensemble model for testing.\nProject Takeaways # Throughout the development of the Recommender System for Book Ratings, I gained several key takeaways:\nEnhanced Python skills, especially in data manipulation, model training, and evaluation. Deepened understanding of collaborative filtering techniques and their application in recommender systems. Explored data exploration techniques to identify trends and patterns in book ratings. Learned data preprocessing methods to improve data quality and reduce training time. Experienced model selection processes using cross-validation to identify the best-performing model. Practiced fine-tuning models with random search cross-validation for improved accuracy and robustness. Constructed an ensemble model by combining the top-performing models, showcasing the benefits of leveraging multiple models. Utilized evaluation metrics (MAE and RMSE) to quantitatively measure the performance of the recommender system. Identified limitations of collaborative filtering, such as cold-start issues and the presence of unseen books. Recognized the potential for future enhancements, including incorporating content-based methods and hybrid approaches. Considered scalability using frameworks like Spark to handle larger datasets and improve system performance.(Future improvement) Usage # The recommender system can be used to provide book recommendations based on user preferences. Users can input their user ID or book name to receive personalized recommendations. The system utilizes collaborative filtering techniques and an ensemble model to generate accurate recommendations.\nTo obtain recommendations for a specific user:\nimport recommender_system book_name = \u0026#34;Angels \u0026amp; Demons (Robert Langdon, #1)\u0026#34; # Name of the book for which recommendations are required recommendations(df = test_data,book_name = book_name) id user_id\trating 33038\t1783\t4 26076\t1483\t4 193\t6\t3 26074\t1541\t3 930\t65\t3 For sample output ,we look at the reading history of top user 1783\ntrain[train[\u0026#39;user_id\u0026#39;] == 1483] user_id item_id rating book_name 135608 1483 399 4 The Da Vinci Code (Robert Langdon, #2) 135673 1483 456 4 Memoirs of a Geisha 135757 1483 1074 5 1984 135885 1483 1113 4 Harry Potter and the Order of the Phoenix (Har\u0026hellip; 136250 1483 1302 5 The Devil in the White City For the 1st place recommendation, since this user readed the Da Vinci Code, they might also interesting in its prequel. So the recommendation logically make sence\n","externalUrl":null,"permalink":"/project/recommender/","section":"Projects","summary":"","title":"Novel Recommender","type":"project"},{"content":" Github link # ðŸ’¾Backend\nCredit # Transformer model is introduced in this paper\nSkill invlove # Python Pytorch Deep Learning\nModel 1:\nHand Coded Transformer with Multihead Self-Attention # This is a hand-coded implementation of Transformer with Multihead Self-Attention.\nThe main goal is to help me better understand the underline logic of transformer model\nOverview # The Transformer model revolutionized sequence modeling by employing the self-attention mechanism. This mechanism enables the model to capture long-range dependencies efficiently, making it highly effective for processing sequential data. The core idea of the Transformer lies in its ability to process input sequences in parallel, resulting in faster computation and better capture of global information.\nThe key component of the Transformer model is the multihead self-attention mechanism. This mechanism allows the model to attend to different parts of the input sequence simultaneously, utilizing multiple attention heads. By leveraging multiple attention heads, the model can learn diverse representations and capture various dependencies within the sequence.\nFeatures # Hand-coded implementation of the Transformer model with multihead self-attention Encoder-decoder architecture for sequence-to-sequence tasks Self-attention mechanism for capturing global dependencies Multihead attention for learning diverse representations Positional encoding to incorporate word order information Feed-forward networks for non-linear transformations Customizable hyperparameters for fine-tuning the model Training procedures for optimizing the model\u0026rsquo;s performance ","externalUrl":null,"permalink":"/posts/hand_coded/","section":"Posts","summary":"","title":"Hand Coded","type":"posts"},{"content":" Github link # ðŸ’¾Backend\nCredit # Transformer model is introduced in this paper\nSkill invlove # Python Pytorch Deep Learning\nModel 1:\nHand Coded Transformer with Multihead Self-Attention # This is a hand-coded implementation of Transformer with Multihead Self-Attention.\nThe main goal is to help me better understand the underline logic of transformer model\nOverview # The Transformer model revolutionized sequence modeling by employing the self-attention mechanism. This mechanism enables the model to capture long-range dependencies efficiently, making it highly effective for processing sequential data. The core idea of the Transformer lies in its ability to process input sequences in parallel, resulting in faster computation and better capture of global information.\nThe key component of the Transformer model is the multihead self-attention mechanism. This mechanism allows the model to attend to different parts of the input sequence simultaneously, utilizing multiple attention heads. By leveraging multiple attention heads, the model can learn diverse representations and capture various dependencies within the sequence.\nFeatures # Hand-coded implementation of the Transformer model with multihead self-attention Encoder-decoder architecture for sequence-to-sequence tasks Self-attention mechanism for capturing global dependencies Multihead attention for learning diverse representations Positional encoding to incorporate word order information Feed-forward networks for non-linear transformations Customizable hyperparameters for fine-tuning the model Training procedures for optimizing the model\u0026rsquo;s performance ","externalUrl":null,"permalink":"/project/hand_coded/","section":"Projects","summary":"","title":"Hand Coded","type":"project"},{"content":" Github link # ðŸ’¾Github\nSkill invlove # Python Pytorch Deep Learning\nDescription # This project aim to build a lightweight data-driven application on dog breed classification using transfer learning, where we try to use small model,and test different hyperparameter to optimize the result.\nFigure 1 # Model loss and accuracy when using RESNET 50 + LION optimizer + LR Scheduler\nThe trend goes as desire ","externalUrl":null,"permalink":"/project/dogbreedclassification/","section":"Projects","summary":"","title":"Dog Breed Classification","type":"project"},{"content":" Github link # ðŸ”— Function URL ðŸ–¥ Code\nCredit # Members of team TP8 for subject FIT5120/22\nSkill invlove # Python Pandas Machine Learning\nDescription # Key Takeaways # Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Agile Management. Here I also exposed to the full agile software development cycle of a product Method to improve the models Where to improve # Use full training set to increase accuracy and F1 perfromance, reducee False positive and Falues Negative Different method to preprocess the data Function Code # # For Data manipulation import pandas as pd import numpy as np # For Data Visulization import matplotlib.pyplot as plt import seaborn as sns #For Modelling and evaluation from sklearn.model_selection import train_test_split,StratifiedKFold, cross_val_score from xgboost import XGBClassifier import lightgbm as lgb from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,StackingClassifier from sklearn import metrics from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.neighbors import KNeighborsClassifier as KNN #For text preprocessing from sklearn.feature_extraction.text import TfidfVectorizer #For model parameter saving and loading import pickle # Import dataset df = pd.read_csv(\u0026#39;data.csv\u0026#39;,on_bad_lines=\u0026#39;skip\u0026#39;) #ignore badline from dataset df.isna().sum() df.duplicated().sum() #only 1 missing value, and no duplicate found,will drop the bad record directly df.dropna(inplace = True) #Import text data for all the weak passwords from rock you leak df2 = pd.read_csv(\u0026#39;rockyou.txt\u0026#39;,delimiter=\u0026#39;\\t\u0026#39;,header = None, names = [\u0026#39;password\u0026#39;],encoding=\u0026#39;ISO-8859-1\u0026#39;) df2.dropna(inplace = True) df2.drop_duplicates(inplace = True) df2[\u0026#39;strength\u0026#39;] = 0 df_full = pd.concat([df,df2],ignore_index=True) # Compute the value counts of the Gender column value_counts = df_full[\u0026#39;strength\u0026#39;].value_counts() # Set the number of samples to be drawn from each group n_samples = value_counts.min() # Group the dataframe by Gender and sample n_samples from each group sampled_df = df.groupby(\u0026#39;strength\u0026#39;).apply(lambda x: x.sample(n=n_samples)).reset_index(drop=True) # Print the sampled dataframe print(sampled_df) X = sampled_df[\u0026#39;password\u0026#39;] y = sampled_df[\u0026#39;strength\u0026#39;] #tokenize password vectorizer = TfidfVectorizer(analyzer = \u0026#39;char\u0026#39;) X = vectorizer.fit_transform(X) #Save the vectorizer for backend use with open(\u0026#34;vectorizer2.pkl\u0026#34;, \u0026#34;wb\u0026#34;) as f: pickle.dump(vectorizer, f) #Data preprocessing def train_val_test_split(X,y,ratio): X_train,X_,y_train,y_ = train_test_split(X,y,test_size=ratio,stratify=y,random_state=1) X_val,X_test,y_val,y_test = train_test_split(X_,y_,test_size=.5,stratify=y_,random_state=1) return X_train,X_val,X_test,y_train,y_val,y_test X_train,X_val,X_test,y_train,y_val,y_test = train_val_test_split(X,y,ratio=.25) #Model Selection with CV models = [] # Empty list to store all the models # Appending models into the list models.append((\u0026#34;Random forest\u0026#34;, RandomForestClassifier(random_state=1))) models.append((\u0026#34;Bagging\u0026#34;, BaggingClassifier(random_state=1))) models.append((\u0026#34;Xgboost\u0026#34;, XGBClassifier(random_state=1, eval_metric=\u0026#34;logloss\u0026#34;))) models.append((\u0026#34;lgbm\u0026#34;, lgb.LGBMClassifier(random_state=1))) models.append((\u0026#39;KNN\u0026#39;,KNN())) results = [] # Empty list to store all model\u0026#39;s CV scores names = [] # Empty list to store name of the models score = [] # loop through all models to get the mean cross validated score print(\u0026#34;\\n\u0026#34; \u0026#34;Cross-Validation Performance:\u0026#34; \u0026#34;\\n\u0026#34;) # Use F1 since for password evaluation I want to ensure to minimal both FP and FN rate for name, model in models: kfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1 ) # Setting number of splits equal to 5 cv_result = cross_val_score( estimator=model, X=X_train, y=y_train, scoring=\u0026#39;f1_macro\u0026#39;, cv=kfold ) results.append(cv_result) names.append(name) print(\u0026#34;{}: {}\u0026#34;.format(name, cv_result.mean())) #Form CV result, XGBoost generate around 98.4% marco f1 score #Using Xgboost provided the best f1_marco result, thus we can fine tune it # defining model - XGBoost Hyperparameter Tuning model = XGBClassifier(random_state=1, eval_metric=\u0026#34;logloss\u0026#34;) # Parameter grid to pass in RandomizedSearchCV param_grid = { \u0026#34;n_estimators\u0026#34;: np.arange(150, 300, 50), \u0026#34;learning_rate\u0026#34;: [0.0001,0.001,0.01,0.0015], \u0026#34;gamma\u0026#34;: [0, 3, 5,7], \u0026#34;subsample\u0026#34;: [0.5, 0.9,0.2,0.35], \u0026#39;reg_alpha\u0026#39;:[0,1], \u0026#39;reg_lambda\u0026#39;:[0,1] } # Calling RandomizedSearchCV randomized_cv = RandomizedSearchCV( estimator=model, param_distributions=param_grid, n_iter=20, # sample 20 setting scoring=\u0026#39;f1_macro\u0026#39;, cv=3, random_state=1, n_jobs=-1, ) # Fitting parameters in RandomizedSearchCV randomized_cv.fit(X_train, y_train) print( \u0026#34;Best parameters are {} with CV score={}:\u0026#34;.format( randomized_cv.best_params_, randomized_cv.best_score_ ) ) best_model = XGBClassifier( **randomized_cv.best_params_,eval_metric = \u0026#39;logloss\u0026#39;,random_state = 1 )# building model with best parameters print(\u0026#34;\\n\u0026#34; \u0026#34;Training Performance:\u0026#34; \u0026#34;\\n\u0026#34;) best_model.fit(X_train, y_train) scores = metrics.f1_score(y_train, best_model.predict(X_train),average=\u0026#39;macro\u0026#39;) print(\u0026#34; {}\u0026#34;.format( scores)) print(\u0026#34;\\n\u0026#34; \u0026#34;Validation Performance:\u0026#34; \u0026#34;\\n\u0026#34;) val_scores = metrics.f1_score(y_val, best_model.predict(X_val),average=\u0026#39;macro\u0026#39;) print(\u0026#34;{}\u0026#34;.format(val_scores)) # We can go ahead test the model if both training and validation performance are as expected y_pred = best_model.predict(X_test) metrics.f1_score(y_test,y_pred,average=\u0026#39;macro\u0026#39;) import pickle # Save the model into the pickle file for backend with open(\u0026#34;xgb_model.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: pickle.dump(best_model,f) ","externalUrl":null,"permalink":"/project/security_check/","section":"Projects","summary":"","title":"Password Strength Validation","type":"project"},{"content":"Data and product specialist with 2 years of experience specializing in leveraging data to drive business success across diverse sectors including Finance, Retail, Logistic and Operation.\nSkilled in uncovering trends and creating data-driven strategies that empower business decision-making and success.\nSKILLS :\nData Analysis Tools: SQL, Python(Pandas,Matplotlib),Spark(Pyspark), Advanced Excel\nMachine Learning: Python(Scikit-Learn,Pytorch),Model Fine-Tuning, Algorithms selection, performance optimization and MLops(Azure ML),Deployment(Azure, FastAPI)\nData Visualization: Power BI\nDatabase Management: Snowflake, Azure Data Solutions\nCloud \u0026amp; ETL: Azure, Snowflake, Data Warehousing, ETL Processes\nMethodologies: Agile (Scrum), Waterfall\nTools: Jira, Git, Confluence\n","externalUrl":null,"permalink":"/myself/","section":"Projects","summary":"","title":"About Myself","type":"page"},{"content":"r\nGithub link # ðŸ’¾Backend\nSkill invlove # Python Pytorch Deep Learning\nDescription # ","externalUrl":null,"permalink":"/project/windpower/","section":"Projects","summary":"","title":"WINDPOWER_PLACEHOLDER","type":"project"},{"content":"","date":"4 January 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"4 January 2025","externalUrl":null,"permalink":"/project/","section":"Projects","summary":"","title":"Projects","type":"project"},{"content":" ","date":"4 January 2025","externalUrl":null,"permalink":"/","section":"Projects","summary":"","title":"Projects","type":"page"},{"content":" Tester article # Description # Managed operations for a 118-room hotel,coordinated and supervised the daily operation Analysed booking patterns and implemented revenue optimization strategies based on demand trends Implemented revenue strategies by dynamically adjusting inventory based on daily demand trends, leading to improved revenue performance. Maximize Average Daily Rate and occupancy Identify opportunities for targeted promotions to ensure profit Conducted regular performance analysis of the reservation system, identifying areas for improvement, and implementing process enhancements. ","date":"4 January 2025","externalUrl":null,"permalink":"/posts/tester/","section":"Posts","summary":"","title":"Tester","type":"posts"},{"content":" Tester article # Description # Managed operations for a 118-room hotel,coordinated and supervised the daily operation Analysed booking patterns and implemented revenue optimization strategies based on demand trends Implemented revenue strategies by dynamically adjusting inventory based on daily demand trends, leading to improved revenue performance. Maximize Average Daily Rate and occupancy Identify opportunities for targeted promotions to ensure profit Conducted regular performance analysis of the reservation system, identifying areas for improvement, and implementing process enhancements. ","date":"4 January 2025","externalUrl":null,"permalink":"/project/tester/","section":"Projects","summary":"","title":"Tester","type":"project"},{"content":"","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"","title":"Advanced","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/bert/","section":"Tags","summary":"","title":"BERT","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/tags/cnn/","section":"Tags","summary":"","title":"CNN","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/collaborative-filtering/","section":"Tags","summary":"","title":"Collaborative Filtering","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/css/","section":"Tags","summary":"","title":"Css","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/django/","section":"Tags","summary":"","title":"Django","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/docs/","section":"Tags","summary":"","title":"Docs","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/documentation/","section":"Series","summary":"","title":"Documentation","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/emsemble-machine-learning/","section":"Tags","summary":"","title":"Emsemble Machine Learning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/pandas/","section":"Tags","summary":"","title":"Pandas","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/recommender/","section":"Tags","summary":"","title":"Recommender","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/seaborn/","section":"Tags","summary":"","title":"Seaborn","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/sklearn/","section":"Tags","summary":"","title":"Sklearn","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/streamlit/","section":"Tags","summary":"","title":"Streamlit","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/xgboost/","section":"Tags","summary":"","title":"Xgboost","type":"tags"}]