
[{"content":"There are many ways you can make advanced changes to Blowfish. Read below to learn more about what can be customised and the best way of achieving your desired result.\nIf you need further advice, post your questions on GitHub Discussions.\nHugo project structure # Before leaping into it, first a quick note about Hugo project structure and best practices for managing your content and theme customisations.\nIn summary: Never directly edit the theme files. Only make customisations in your Hugo project\u0026rsquo;s sub-directories, not in the themes directory itself. Blowfish is built to take advantage of all the standard Hugo practices. It is designed to allow all aspects of the theme to be customised and overridden without changing any of the core theme files. This allows for a seamless upgrade experience while giving you total control over the look and feel of your website.\nIn order to achieve this, you should never manually adjust any of the theme files directly. Whether you install using Hugo modules, as a git submodule or manually include the theme in your themes/ directory, you should always leave these files intact.\nThe correct way to adjust any theme behaviour is by overriding files using Hugo\u0026rsquo;s powerful file lookup order. In summary, the lookup order ensures any files you include in your project directory will automatically take precedence over any theme files.\nFor example, if you wanted to override the main article template in Blowfish, you can simply create your own layouts/_default/single.html file and place it in the root of your project. This file will then override the single.html from the theme without ever changing the theme itself. This works for any theme files - HTML templates, partials, shortcodes, config files, data, assets, etc.\nAs long as you follow this simple practice, you will always be able to update the theme (or test different theme versions) without worrying that you will lose any of your custom changes.\nChange image optimization settings # Hugo has various builtin methods to resize, crop and optimize images.\nAs an example - in layouts/partials/article-link/card.html, you have the following code:\n{{ with .Resize \u0026#34;600x\u0026#34; }} \u0026lt;div class=\u0026#34;w-full thumbnail_card nozoom\u0026#34; style=\u0026#34;background-image:url({{ .RelPermalink }});\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; {{ end }} The default behavior of Hugo here is to resize the image to 600px keeping the ratio.\nIt is worth noting here that default image configurations such as anchor point can also be set in your site configuration as well as in the template itself.\nSee the Hugo docs on image processing for more info.\nColour schemes # In addition to the default schemes, you can also create your own and re-style the entire website to your liking. Schemes are created by by placing a \u0026lt;scheme-name\u0026gt;.css file in the assets/css/schemes/ folder. Once the file is created, simply refer to it by name in the theme configuration.\nNote: generating these files manually can be hard, I\u0026rsquo;ve built a nodejs terminal tool to help with that, Fugu. In a nutshell, you pass the main three hex values of your color palette and the program will output a css file that can be imported directly into Blowfish. Blowfish defines a three-colour palette that is used throughout the theme. The three colours are defined as neutral, primary and secondary variants, each containing ten shades of colour.\nDue to the way Tailwind CSS 3.0 calculates colour values with opacity, the colours specified in the scheme need to conform to a particular format by providing the red, green and blue colour values.\n:root { --color-primary-500: 139, 92, 246; } This example defines a CSS variable for the primary-500 colour with a red value of 139, green value of 92 and blue value of 246.\nUse one of the existing theme stylesheets as a template. You are free to define your own colours, but for some inspiration, check out the official Tailwind colour palette reference.\nOverriding the stylesheet # Sometimes you need to add a custom style to style your own HTML elements. Blowfish provides for this scenario by allowing you to override the default styles in your own CSS stylesheet. Simply create a custom.css file in your project\u0026rsquo;s assets/css/ folder.\nThe custom.css file will be minified by Hugo and loaded automatically after all the other theme styles which means anything in your custom file will take precedence over the defaults.\nUsing additional fonts # Blowfish allows you to easily change the font for your site. After creating a custom.css file in your project\u0026rsquo;s assets/css/ folder, place you font file inside a fonts folder within the static root folder.\n. ├── assets │ └── css │ └── custom.css ... └─── static └── fonts └─── font.ttf This makes the font available to the website. Now, the font can just import it in your custom.css and replaced wherever you see fit. The example below shows what replacing the font for the entire html would look like.\n@font-face { font-family: font; src: url(\u0026#39;/fonts/font.ttf\u0026#39;); } html { font-family: font; } Adjusting the font size # Changing the font size of your website is one example of overriding the default stylesheet. Blowfish makes this simple as it uses scaled font sizes throughout the theme which are derived from the base HTML font size. By default, Tailwind sets the default size to 12pt, but it can be changed to whatever value you prefer.\n/* Increase the default font size */ html { font-size: 13pt; } Simply by changing this one value, all the font sizes on your website will be adjusted to match this new size. Therefore, to increase the overall font sizes used, make the value greater than 12pt. Similarly, to decrease the font sizes, make the value less than 12pt.\nBuilding the theme CSS from source # If you\u0026rsquo;d like to make a major change, you can take advantage of Tailwind CSS\u0026rsquo;s JIT compiler and rebuild the entire theme CSS from scratch. This is useful if you want to adjust the Tailwind configuration or add extra Tailwind classes to the main stylesheet.\nNote: Building the theme manually is intended for advanced users. Let\u0026rsquo;s step through how building the Tailwind CSS works.\nTailwind configuration # In order to generate a CSS file that only contains the Tailwind classes that are actually being used the JIT compiler needs to scan through all the HTML templates and Markdown content files to check which styles are present in the markup. The compiler does this by looking at the tailwind.config.js file which is included in the root of the theme directory:\n// themes/blowfish/tailwind.config.js module.exports = { content: [ \u0026#34;./layouts/**/*.html\u0026#34;, \u0026#34;./content/**/*.{html,md}\u0026#34;, \u0026#34;./themes/blowfish/layouts/**/*.html\u0026#34;, \u0026#34;./themes/blowfish/content/**/*.{html,md}\u0026#34;, ], // and more... }; This default configuration has been included with these content paths so that you can easily generate your own CSS file without needing to modify it, provided you follow a particular project structure. Namely, you have to include Blowfish in your project as a subdirectory at themes/blowfish/. This means you cannot easily use Hugo Modules to install the theme and you must go down either the git submodule (recommended) or manual install routes. The explain how to install the theme using either of these methods.\nProject structure # In order to take advantage of the default configuration, your project should look something like this\u0026hellip;\n. ├── assets │ └── css │ └── compiled │ └── main.css # this is the file we will generate ├── config # site config │ └── _default ├── content # site content │ ├── _index.md │ ├── projects │ │ └── _index.md │ └── blog │ └── _index.md ├── layouts # custom layouts for your site │ ├── partials │ │ └── extend-article-link/simple.html │ ├── projects │ │ └── list.html │ └── shortcodes │ └── disclaimer.html └── themes └── blowfish # git submodule or manual theme install This example structure adds a new projects content type with its own custom layout along with a custom shortcode and extended partial. Provided the project follows this structure, all that\u0026rsquo;s required is to recompile the main.css file.\nInstall dependencies # In order for this to work you\u0026rsquo;ll need to change into the themes/blowfish/ directory and install the project dependencies. You\u0026rsquo;ll need npm on your local machine for this step.\ncd themes/blowfish npm install Run the Tailwind compiler # With the dependencies installed all that\u0026rsquo;s left is to use Tailwind CLI to invoke the JIT compiler. Navigate back to the root of your Hugo project and issue the following command:\ncd ../.. ./themes/blowfish/node_modules/tailwindcss/lib/cli.js -c ./themes/blowfish/tailwind.config.js -i ./themes/blowfish/assets/css/main.css -o ./assets/css/compiled/main.css --jit It\u0026rsquo;s a bit of an ugly command due to the paths involved but essentially you\u0026rsquo;re calling Tailwind CLI and passing it the location of the Tailwind config file (the one we looked at above), where to find the theme\u0026rsquo;s main.css file and then where you want the compiled CSS file to be placed (it\u0026rsquo;s going into the assets/css/compiled/ folder of your Hugo project).\nThe config file will automatically inspect all the content and layouts in your project as well as all those in the theme and build a new CSS file that contains all the CSS required for your website. Due to the way Hugo handles file hierarchy, this file in your project will now automatically override the one that comes with the theme.\nEach time you make a change to your layouts and need new Tailwind CSS styles, you can simply re-run the command and generate the new CSS file. You can also add -w to the end of the command to run the JIT compiler in watch mode.\nMake a build script # To fully complete this solution, you can simplify this whole process by adding aliases for these commands, or do what I do and add a package.json to the root of your project which contains the necessary scripts\u0026hellip;\n// package.json { \u0026#34;name\u0026#34;: \u0026#34;my-website\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;hugo server -b http://localhost -p 8000\u0026#34;, \u0026#34;dev\u0026#34;: \u0026#34;NODE_ENV=development ./themes/blowfish/node_modules/tailwindcss/lib/cli.js -c ./themes/blowfish/tailwind.config.js -i ./themes/blowfish/assets/css/main.css -o ./assets/css/compiled/main.css --jit -w\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;NODE_ENV=production ./themes/blowfish/node_modules/tailwindcss/lib/cli.js -c ./themes/blowfish/tailwind.config.js -i ./themes/blowfish/assets/css/main.css -o ./assets/css/compiled/main.css --jit\u0026#34; }, // and more... } Now when you want to work on designing your site, you can invoke npm run dev and the compiler will run in watch mode. When you\u0026rsquo;re ready to deploy, run npm run build and you\u0026rsquo;ll get a clean Tailwind CSS build.\n🙋‍♀️ If you need help, feel free to ask a question on GitHub Discussions.\n","externalUrl":null,"permalink":"/project/advanced-customisation/","section":"Projects","summary":"","title":"Advanced Customisation","type":"project"},{"content":" Github link # 💾 Project Github\nCredit # Members of team TP8 for subject FIT5120/22\nSkill invlove # Python Pytorch Deep Learning Docker AWS Azure\nDescription # This is a automated task of determining whether a piece of text is a spam or not. In this project, I built a classifier using PyTorch to fine-tune a BERT model.\nKey Takeaways # Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Collaborative Development. Experience a full-stack project development with a team,and using Github for version control in a team setting Agile Management. Here I also exposed to the full agile software development cycle of a product Cloud Computing. In-depth knowledge and hands on experience on both AWS(EC2,Route52,S3) and Azure (Machine Learning Studio,Virtual machines) New Domain knowledge. First time expose to developing a product in a more public domain setting ","externalUrl":null,"permalink":"/project/seniordigi/","section":"Projects","summary":"","title":"Bridging the Digital Divide","type":"project"},{"content":" Github link # 💾 Project Github\nSkill invlove # Python Machine Learning Data Visulization\nDescription # The increasing number of VISA applicants every year in US calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. The objective is to analyze the data provided and, with the help of a classification model:\nFacilitate the process of visa approvals. Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status. Figure 1 # Important feature that affect the application success rate - either positive or negative Figure 2 # Important feature that affect the application success rate - either positive or negative Key Takeaways # End to End data project involve data exploration, data manipulation and preprocessing, model selection, evaluation and fine-tune. ","externalUrl":null,"permalink":"/project/visa-granted-rate/","section":"Projects","summary":"","title":"Visa Approveal rate predictionm","type":"project"},{"content":" Github link # 🔗 Code\nCredit # This notebook is inspire by Paper Skill invlove # Python Collaborative Filtering Data preprocessing ScikitLearn\nProject Description # This project aims to build a recommender system for book ratings. The system utilizes collaboration filtering methods and ensemble techniques to provide accurate recommendations to users based on their preferences. The project includes data exploration, preprocessing, model selection, model construction, and evaluation phases.\nThe recommender system employs three standalone models, implementing collaboration filtering methods. The models are fine-tuned and compared using cross-validation to select the best-performing model. The chosen model is then further optimized using random search cross-validation. The top-performing models, including SVD, SVDpp, and KNNwithZscore, are used to construct an ensemble model for testing.\nProject Takeaways # Throughout the development of the Recommender System for Book Ratings, I gained several key takeaways:\nEnhanced Python skills, especially in data manipulation, model training, and evaluation. Deepened understanding of collaborative filtering techniques and their application in recommender systems. Explored data exploration techniques to identify trends and patterns in book ratings. Learned data preprocessing methods to improve data quality and reduce training time. Experienced model selection processes using cross-validation to identify the best-performing model. Practiced fine-tuning models with random search cross-validation for improved accuracy and robustness. Constructed an ensemble model by combining the top-performing models, showcasing the benefits of leveraging multiple models. Utilized evaluation metrics (MAE and RMSE) to quantitatively measure the performance of the recommender system. Identified limitations of collaborative filtering, such as cold-start issues and the presence of unseen books. Recognized the potential for future enhancements, including incorporating content-based methods and hybrid approaches. Considered scalability using frameworks like Spark to handle larger datasets and improve system performance.(Future improvement) Usage # The recommender system can be used to provide book recommendations based on user preferences. Users can input their user ID or book name to receive personalized recommendations. The system utilizes collaborative filtering techniques and an ensemble model to generate accurate recommendations.\nTo obtain recommendations for a specific user:\nimport recommender_system book_name = \u0026#34;Angels \u0026amp; Demons (Robert Langdon, #1)\u0026#34; # Name of the book for which recommendations are required recommendations(df = test_data,book_name = book_name) id user_id\trating 33038\t1783\t4 26076\t1483\t4 193\t6\t3 26074\t1541\t3 930\t65\t3 For sample output ,we look at the reading history of top user 1783\ntrain[train[\u0026#39;user_id\u0026#39;] == 1483] user_id item_id rating book_name 135608 1483 399 4 The Da Vinci Code (Robert Langdon, #2) 135673 1483 456 4 Memoirs of a Geisha 135757 1483 1074 5 1984 135885 1483 1113 4 Harry Potter and the Order of the Phoenix (Har\u0026hellip; 136250 1483 1302 5 The Devil in the White City For the 1st place recommendation, since this user readed the Da Vinci Code, they might also interesting in its prequel. So the recommendation logically make sence\n","externalUrl":null,"permalink":"/posts/recommender/","section":"Posts","summary":"","title":"Novel Recommender","type":"posts"},{"content":" Github link # 🔗 Code\nCredit # This notebook is inspire by Paper Skill invlove # Python Collaborative Filtering Data preprocessing ScikitLearn\nProject Description # This project aims to build a recommender system for book ratings. The system utilizes collaboration filtering methods and ensemble techniques to provide accurate recommendations to users based on their preferences. The project includes data exploration, preprocessing, model selection, model construction, and evaluation phases.\nThe recommender system employs three standalone models, implementing collaboration filtering methods. The models are fine-tuned and compared using cross-validation to select the best-performing model. The chosen model is then further optimized using random search cross-validation. The top-performing models, including SVD, SVDpp, and KNNwithZscore, are used to construct an ensemble model for testing.\nProject Takeaways # Throughout the development of the Recommender System for Book Ratings, I gained several key takeaways:\nEnhanced Python skills, especially in data manipulation, model training, and evaluation. Deepened understanding of collaborative filtering techniques and their application in recommender systems. Explored data exploration techniques to identify trends and patterns in book ratings. Learned data preprocessing methods to improve data quality and reduce training time. Experienced model selection processes using cross-validation to identify the best-performing model. Practiced fine-tuning models with random search cross-validation for improved accuracy and robustness. Constructed an ensemble model by combining the top-performing models, showcasing the benefits of leveraging multiple models. Utilized evaluation metrics (MAE and RMSE) to quantitatively measure the performance of the recommender system. Identified limitations of collaborative filtering, such as cold-start issues and the presence of unseen books. Recognized the potential for future enhancements, including incorporating content-based methods and hybrid approaches. Considered scalability using frameworks like Spark to handle larger datasets and improve system performance.(Future improvement) Usage # The recommender system can be used to provide book recommendations based on user preferences. Users can input their user ID or book name to receive personalized recommendations. The system utilizes collaborative filtering techniques and an ensemble model to generate accurate recommendations.\nTo obtain recommendations for a specific user:\nimport recommender_system book_name = \u0026#34;Angels \u0026amp; Demons (Robert Langdon, #1)\u0026#34; # Name of the book for which recommendations are required recommendations(df = test_data,book_name = book_name) id user_id\trating 33038\t1783\t4 26076\t1483\t4 193\t6\t3 26074\t1541\t3 930\t65\t3 For sample output ,we look at the reading history of top user 1783\ntrain[train[\u0026#39;user_id\u0026#39;] == 1483] user_id item_id rating book_name 135608 1483 399 4 The Da Vinci Code (Robert Langdon, #2) 135673 1483 456 4 Memoirs of a Geisha 135757 1483 1074 5 1984 135885 1483 1113 4 Harry Potter and the Order of the Phoenix (Har\u0026hellip; 136250 1483 1302 5 The Devil in the White City For the 1st place recommendation, since this user readed the Da Vinci Code, they might also interesting in its prequel. So the recommendation logically make sence\n","externalUrl":null,"permalink":"/project/recommender/","section":"Projects","summary":"","title":"Novel Recommender","type":"project"},{"content":" Github link # 💾Backend\nCredit # Transformer model is introduced in this paper\nSkill invlove # Python Pytorch Deep Learning\nModel 1:\nHand Coded Transformer with Multihead Self-Attention # This is a hand-coded implementation of Transformer with Multihead Self-Attention.\nThe main goal is to help me better understand the underline logic of transformer model\nOverview # The Transformer model revolutionized sequence modeling by employing the self-attention mechanism. This mechanism enables the model to capture long-range dependencies efficiently, making it highly effective for processing sequential data. The core idea of the Transformer lies in its ability to process input sequences in parallel, resulting in faster computation and better capture of global information.\nThe key component of the Transformer model is the multihead self-attention mechanism. This mechanism allows the model to attend to different parts of the input sequence simultaneously, utilizing multiple attention heads. By leveraging multiple attention heads, the model can learn diverse representations and capture various dependencies within the sequence.\nFeatures # Hand-coded implementation of the Transformer model with multihead self-attention Encoder-decoder architecture for sequence-to-sequence tasks Self-attention mechanism for capturing global dependencies Multihead attention for learning diverse representations Positional encoding to incorporate word order information Feed-forward networks for non-linear transformations Customizable hyperparameters for fine-tuning the model Training procedures for optimizing the model\u0026rsquo;s performance ","externalUrl":null,"permalink":"/posts/hand_coded/","section":"Posts","summary":"","title":"Hand Coded","type":"posts"},{"content":" Github link # 💾Backend\nCredit # Transformer model is introduced in this paper\nSkill invlove # Python Pytorch Deep Learning\nModel 1:\nHand Coded Transformer with Multihead Self-Attention # This is a hand-coded implementation of Transformer with Multihead Self-Attention.\nThe main goal is to help me better understand the underline logic of transformer model\nOverview # The Transformer model revolutionized sequence modeling by employing the self-attention mechanism. This mechanism enables the model to capture long-range dependencies efficiently, making it highly effective for processing sequential data. The core idea of the Transformer lies in its ability to process input sequences in parallel, resulting in faster computation and better capture of global information.\nThe key component of the Transformer model is the multihead self-attention mechanism. This mechanism allows the model to attend to different parts of the input sequence simultaneously, utilizing multiple attention heads. By leveraging multiple attention heads, the model can learn diverse representations and capture various dependencies within the sequence.\nFeatures # Hand-coded implementation of the Transformer model with multihead self-attention Encoder-decoder architecture for sequence-to-sequence tasks Self-attention mechanism for capturing global dependencies Multihead attention for learning diverse representations Positional encoding to incorporate word order information Feed-forward networks for non-linear transformations Customizable hyperparameters for fine-tuning the model Training procedures for optimizing the model\u0026rsquo;s performance ","externalUrl":null,"permalink":"/project/hand_coded/","section":"Projects","summary":"","title":"Hand Coded","type":"project"},{"content":" Github link # 💾Github\nSkill invlove # Python Pytorch Deep Learning\nDescription # This project aim to build a lightweight data-driven application on dog breed classification using transfer learning, where we try to use small model,and test different hyperparameter to optimize the result.\nFigure 1 # Model loss and accuracy when using RESNET 50 + LION optimizer + LR Scheduler\nThe trend goes as desire ","externalUrl":null,"permalink":"/project/dogbreedclassification/","section":"Projects","summary":"","title":"Dog Breed Classification","type":"project"},{"content":" Github link # 🔗 Function URL 🖥 Code\nCredit # Members of team TP8 for subject FIT5120/22\nSkill invlove # Python Pandas Machine Learning\nDescription # Key Takeaways # Combining DS skill to a real project. Experience on merging model develpment with backend(Django) and frontend(React). Agile Management. Here I also exposed to the full agile software development cycle of a product Method to improve the models Where to improve # Use full training set to increase accuracy and F1 perfromance, reducee False positive and Falues Negative Different method to preprocess the data Function Code # # For Data manipulation import pandas as pd import numpy as np # For Data Visulization import matplotlib.pyplot as plt import seaborn as sns #For Modelling and evaluation from sklearn.model_selection import train_test_split,StratifiedKFold, cross_val_score from xgboost import XGBClassifier import lightgbm as lgb from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,StackingClassifier from sklearn import metrics from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.neighbors import KNeighborsClassifier as KNN #For text preprocessing from sklearn.feature_extraction.text import TfidfVectorizer #For model parameter saving and loading import pickle # Import dataset df = pd.read_csv(\u0026#39;data.csv\u0026#39;,on_bad_lines=\u0026#39;skip\u0026#39;) #ignore badline from dataset df.isna().sum() df.duplicated().sum() #only 1 missing value, and no duplicate found,will drop the bad record directly df.dropna(inplace = True) #Import text data for all the weak passwords from rock you leak df2 = pd.read_csv(\u0026#39;rockyou.txt\u0026#39;,delimiter=\u0026#39;\\t\u0026#39;,header = None, names = [\u0026#39;password\u0026#39;],encoding=\u0026#39;ISO-8859-1\u0026#39;) df2.dropna(inplace = True) df2.drop_duplicates(inplace = True) df2[\u0026#39;strength\u0026#39;] = 0 df_full = pd.concat([df,df2],ignore_index=True) # Compute the value counts of the Gender column value_counts = df_full[\u0026#39;strength\u0026#39;].value_counts() # Set the number of samples to be drawn from each group n_samples = value_counts.min() # Group the dataframe by Gender and sample n_samples from each group sampled_df = df.groupby(\u0026#39;strength\u0026#39;).apply(lambda x: x.sample(n=n_samples)).reset_index(drop=True) # Print the sampled dataframe print(sampled_df) X = sampled_df[\u0026#39;password\u0026#39;] y = sampled_df[\u0026#39;strength\u0026#39;] #tokenize password vectorizer = TfidfVectorizer(analyzer = \u0026#39;char\u0026#39;) X = vectorizer.fit_transform(X) #Save the vectorizer for backend use with open(\u0026#34;vectorizer2.pkl\u0026#34;, \u0026#34;wb\u0026#34;) as f: pickle.dump(vectorizer, f) #Data preprocessing def train_val_test_split(X,y,ratio): X_train,X_,y_train,y_ = train_test_split(X,y,test_size=ratio,stratify=y,random_state=1) X_val,X_test,y_val,y_test = train_test_split(X_,y_,test_size=.5,stratify=y_,random_state=1) return X_train,X_val,X_test,y_train,y_val,y_test X_train,X_val,X_test,y_train,y_val,y_test = train_val_test_split(X,y,ratio=.25) #Model Selection with CV models = [] # Empty list to store all the models # Appending models into the list models.append((\u0026#34;Random forest\u0026#34;, RandomForestClassifier(random_state=1))) models.append((\u0026#34;Bagging\u0026#34;, BaggingClassifier(random_state=1))) models.append((\u0026#34;Xgboost\u0026#34;, XGBClassifier(random_state=1, eval_metric=\u0026#34;logloss\u0026#34;))) models.append((\u0026#34;lgbm\u0026#34;, lgb.LGBMClassifier(random_state=1))) models.append((\u0026#39;KNN\u0026#39;,KNN())) results = [] # Empty list to store all model\u0026#39;s CV scores names = [] # Empty list to store name of the models score = [] # loop through all models to get the mean cross validated score print(\u0026#34;\\n\u0026#34; \u0026#34;Cross-Validation Performance:\u0026#34; \u0026#34;\\n\u0026#34;) # Use F1 since for password evaluation I want to ensure to minimal both FP and FN rate for name, model in models: kfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=1 ) # Setting number of splits equal to 5 cv_result = cross_val_score( estimator=model, X=X_train, y=y_train, scoring=\u0026#39;f1_macro\u0026#39;, cv=kfold ) results.append(cv_result) names.append(name) print(\u0026#34;{}: {}\u0026#34;.format(name, cv_result.mean())) #Form CV result, XGBoost generate around 98.4% marco f1 score #Using Xgboost provided the best f1_marco result, thus we can fine tune it # defining model - XGBoost Hyperparameter Tuning model = XGBClassifier(random_state=1, eval_metric=\u0026#34;logloss\u0026#34;) # Parameter grid to pass in RandomizedSearchCV param_grid = { \u0026#34;n_estimators\u0026#34;: np.arange(150, 300, 50), \u0026#34;learning_rate\u0026#34;: [0.0001,0.001,0.01,0.0015], \u0026#34;gamma\u0026#34;: [0, 3, 5,7], \u0026#34;subsample\u0026#34;: [0.5, 0.9,0.2,0.35], \u0026#39;reg_alpha\u0026#39;:[0,1], \u0026#39;reg_lambda\u0026#39;:[0,1] } # Calling RandomizedSearchCV randomized_cv = RandomizedSearchCV( estimator=model, param_distributions=param_grid, n_iter=20, # sample 20 setting scoring=\u0026#39;f1_macro\u0026#39;, cv=3, random_state=1, n_jobs=-1, ) # Fitting parameters in RandomizedSearchCV randomized_cv.fit(X_train, y_train) print( \u0026#34;Best parameters are {} with CV score={}:\u0026#34;.format( randomized_cv.best_params_, randomized_cv.best_score_ ) ) best_model = XGBClassifier( **randomized_cv.best_params_,eval_metric = \u0026#39;logloss\u0026#39;,random_state = 1 )# building model with best parameters print(\u0026#34;\\n\u0026#34; \u0026#34;Training Performance:\u0026#34; \u0026#34;\\n\u0026#34;) best_model.fit(X_train, y_train) scores = metrics.f1_score(y_train, best_model.predict(X_train),average=\u0026#39;macro\u0026#39;) print(\u0026#34; {}\u0026#34;.format( scores)) print(\u0026#34;\\n\u0026#34; \u0026#34;Validation Performance:\u0026#34; \u0026#34;\\n\u0026#34;) val_scores = metrics.f1_score(y_val, best_model.predict(X_val),average=\u0026#39;macro\u0026#39;) print(\u0026#34;{}\u0026#34;.format(val_scores)) # We can go ahead test the model if both training and validation performance are as expected y_pred = best_model.predict(X_test) metrics.f1_score(y_test,y_pred,average=\u0026#39;macro\u0026#39;) import pickle # Save the model into the pickle file for backend with open(\u0026#34;xgb_model.pkl\u0026#34;, \u0026#34;rb\u0026#34;) as f: pickle.dump(best_model,f) ","externalUrl":null,"permalink":"/project/security_check/","section":"Projects","summary":"","title":"Password Strength Validation","type":"project"},{"content":"Data and product specialist with 2 years of experience specializing in leveraging data to drive business success across diverse sectors including Finance, Retail, Logistic and Operation.\nSkilled in uncovering trends and creating data-driven strategies that empower business decision-making and success.\nSKILLS :\nData Analysis Tools: SQL, Python(Pandas,Matplotlib),Spark(Pyspark), Advanced Excel\nMachine Learning: Python(Scikit-Learn,Pytorch),Model Fine-Tuning, Algorithms selection, performance optimization and MLops(Azure ML),Deployment(Azure, FastAPI)\nData Visualization: Power BI\nDatabase Management: Snowflake, Azure Data Solutions\nCloud \u0026amp; ETL: Azure, Snowflake, Data Warehousing, ETL Processes\nMethodologies: Agile (Scrum), Waterfall\nTools: Confluence, Jira, Git\n","externalUrl":null,"permalink":"/myself/","section":"Projects","summary":"","title":"About Myself","type":"page"},{"content":"r\nGithub link # 💾Backend\nSkill invlove # Python Pytorch Deep Learning\nDescription # ","externalUrl":null,"permalink":"/project/windpower/","section":"Projects","summary":"","title":"WINDPOWER_PLACEHOLDER","type":"project"},{"content":"","date":"4 January 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"4 January 2025","externalUrl":null,"permalink":"/project/","section":"Projects","summary":"","title":"Projects","type":"project"},{"content":"This section contains all my current projects.\n","date":"4 January 2025","externalUrl":null,"permalink":"/","section":"Projects","summary":"","title":"Projects","type":"page"},{"content":" Tester article # Description # Managed operations for a 118-room hotel,coordinated and supervised the daily operation Analysed booking patterns and implemented revenue optimization strategies based on demand trends Implemented revenue strategies by dynamically adjusting inventory based on daily demand trends, leading to improved revenue performance. Maximize Average Daily Rate and occupancy Identify opportunities for targeted promotions to ensure profit Conducted regular performance analysis of the reservation system, identifying areas for improvement, and implementing process enhancements. ","date":"4 January 2025","externalUrl":null,"permalink":"/posts/tester/","section":"Posts","summary":"","title":"Tester","type":"posts"},{"content":" Tester article # Description # Managed operations for a 118-room hotel,coordinated and supervised the daily operation Analysed booking patterns and implemented revenue optimization strategies based on demand trends Implemented revenue strategies by dynamically adjusting inventory based on daily demand trends, leading to improved revenue performance. Maximize Average Daily Rate and occupancy Identify opportunities for targeted promotions to ensure profit Conducted regular performance analysis of the reservation system, identifying areas for improvement, and implementing process enhancements. ","date":"4 January 2025","externalUrl":null,"permalink":"/project/tester/","section":"Projects","summary":"","title":"Tester","type":"project"},{"content":"","externalUrl":null,"permalink":"/tags/advanced/","section":"Tags","summary":"","title":"Advanced","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/bert/","section":"Tags","summary":"","title":"BERT","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/tags/cnn/","section":"Tags","summary":"","title":"CNN","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/collaborative-filtering/","section":"Tags","summary":"","title":"Collaborative Filtering","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/css/","section":"Tags","summary":"","title":"Css","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/django/","section":"Tags","summary":"","title":"Django","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/docs/","section":"Tags","summary":"","title":"Docs","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/documentation/","section":"Series","summary":"","title":"Documentation","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/emsemble-machine-learning/","section":"Tags","summary":"","title":"Emsemble Machine Learning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/pandas/","section":"Tags","summary":"","title":"Pandas","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/recommender/","section":"Tags","summary":"","title":"Recommender","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/seaborn/","section":"Tags","summary":"","title":"Seaborn","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/sklearn/","section":"Tags","summary":"","title":"Sklearn","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/streamlit/","section":"Tags","summary":"","title":"Streamlit","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/xgboost/","section":"Tags","summary":"","title":"Xgboost","type":"tags"}]